{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "506be12a",
   "metadata": {},
   "source": [
    "# Final Project: Huggingface Transformers를 이용한 한-영 번역"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f239a883",
   "metadata": {},
   "source": [
    "## 개요\n",
    "- Assignment2의 연장선에서 한-영 번역 프로그램을 Huggingface의 Transformers와 Dataset을 이용해서 필요한 크기로 데이터를 줄이고 이를 모델에서 파인튜닝하여 번역 성능을 살펴보고 실제 예문의 번역을 출력\n",
    "- [Huggingface NLP course의 7장 Translation](https://huggingface.co/learn/nlp-course/chapter7/4?fw=pt)을 근간으로 해서 (거의 그대로 활용할 수 있음) 구현할 수 있음\n",
    "- Dataset을 자료를 받아서 필요한 크기로 나누고, 학습에 필요한 형태로 Dataset을 재구조화하고 tokenize하는 모듈을 구현\n",
    "- 공개된 자료를 바탕으로 구현하기 때문에 성능보다는 전체 번역모듈을 Huggingface로 구현해보는 것을 주목표로 하기 때문에 완결성이 있어야 하며, 실제로 작동해야 함.\n",
    "- FinalProject_학번_이름.ipynb\n",
    "- Due 12월 8일 11시 59분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516bc721",
   "metadata": {},
   "source": [
    "## 필요한 모듈 설치\n",
    "- 프로그램 실행에 필요한 모듈, Huggingface, Dataset 등을 각자 알아서 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462b32c2",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "- Huggingface Hub에 있는 Dataset 중 `bongsoo/news_talk_en_ko` 는 한국어-영어뉴스 기사를 병렬로 작성한 130만 개의 데이터 셋이다.\n",
    "- 이 데이터셋을 읽어서 colab에서 돌릴 수 있게, training, validation, test 데이터로 각각 120,000, 9,000, 1,000으로 줄여서 학습에 필요한 구조로 만듬\n",
    "- 데이터를 자를때 순차적으로 자르지 말고 전체 데이터를 셔플한 후 필요한 크기로 자를 것\n",
    "- 데이터셋을 pandas 형식으로 받은 후 할 수도 있고 여러 가능한 방법이 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0793719a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732c04a24a2048db8d65f3ec8a11da1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/89.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05669a3340134f349fb03401199094de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/345M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039b25ed92a24442a5696fd36610ab60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1299999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: [\"Skinner's reward is mostly eye-watering.\", '스키너가 말한 보상은 대부분 눈으로 볼 수 있는 현물이다.'],\n",
       "        num_rows: 1299999\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 필요한 라이브러리 설치 및 로드\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터셋 로드 및 처리\n",
    "dataset = load_dataset(\"bongsoo/news_talk_en_ko\")\n",
    "df = pd.DataFrame(dataset['train']).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 데이터 크기 설정\n",
    "train_df = df.iloc[:120_000]\n",
    "val_df = df.iloc[120_000:129_000]\n",
    "test_df = df.iloc[129_000:130_000]\n",
    "\n",
    "# # 데이터 저장\n",
    "# train_df.to_csv(\"train.csv\", index=False)\n",
    "# val_df.to_csv(\"val.csv\", index=False)\n",
    "# test_df.to_csv(\"test.csv\", index=False)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "valid_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293a2864",
   "metadata": {},
   "source": [
    "## Huggingface\n",
    "- 학습에 필요한 Huggingface 모델 사용\n",
    "- AutoTokenizer, AutoModelForSeq2SeqLM 등을 사용\n",
    "- 학습에 사용할 모델은 [T5](https://github.com/AIRC-KETI/ke-t5)(\"KETI-AIR/ke-t5-base\")를 사용할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439be716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface 모델 및 토크나이저 로드\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# 모델 이름 설정\n",
    "model_name = \"KETI-AIR/ke-t5-base\"\n",
    "\n",
    "# 토크나이저와 모델 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# 모델과 토크나이저 준비 완료 확인\n",
    "print(f\"Model {model_name} and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531c6074",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "- T5는 sentencepiece tokenizer를 사용하기 때문에 한-영 병렬 데이터의 자료를 학습시키기 위해서는 이 데이터를 tokenizer를 써서 프로세싱을 해야 한다. 이를 위한 모듈을 만들고 한국어, 영어데이터를 tokenize하여 모델에 입력할 수 있는 형태로(tokenized-dataset) 바꾼다\n",
    "- 이를 위해서 Dataset의 map()을 활용하도록 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8843b8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # 토큰화: source와 target 각각에 대해 처리\n",
    "    inputs = examples[\"ko\"]  # 한국어 데이터\n",
    "    targets = examples[\"en\"]  # 영어 데이터\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
    "    \n",
    "    # 레이블 토큰화\n",
    "    labels = tokenizer(targets, max_length=512, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True, num_proc = 2)\n",
    "valid_dataset = valid_dataset.map(preprocess_function, batched=True, num_proc = 2)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True, num_proc = 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784729dc",
   "metadata": {},
   "source": [
    "## Model\n",
    "- 학습에 필요한 모델 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8425af84",
   "metadata": {},
   "source": [
    "## Collator\n",
    "- 학습할 자료를 정렬하고 모델에 배치 단위로 넘겨주기 위해 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134ac25e",
   "metadata": {},
   "source": [
    "## Metric\n",
    "- 학습한 모델을 측정할 매트릭을 준비\n",
    "- 번역 모델에서는 주로 BLEU 점수를 사용\n",
    "- BLEU 점수는 번역기가 생성한 문장이 레퍼런스(정답이라는 표현을 사용하지 않는 이유는 제대로 된 번역 문장이 오직 하나가 아니기 때문)문장과 얼마나 비슷한지 측정하는 점수\n",
    "\n",
    "- sacrebleu 라이브러리는 BLEU 구현체에서 사실상 표준 라이브러리이며 각 모델이 다른 토크나이저를 쓰는 경우 이를 BPE로 통일 시켜 BLEU 점수를 계산\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65ce175",
   "metadata": {},
   "source": [
    "## 모델 학습(Train)\n",
    "- 학습을 간단히 하기위해 허깅페이스에서 제공하는 Seq2SeqTrainer클래스와 학습 세부 조건은 Seq2SeqTrainingArguments를 활용할 수 있으나, 본 과제에서는 이를 쓰지 말고 Training를 직접 구현하도록 한다. Dataloader, Scheduler, ACCELERATOR, Optimizer 등을 설정하고 실제로 training loop를 돌려서 학습하고, evaluation 데이터로 성능을 검증\n",
    "- colab에서 돌리기 위해서는 성능이 저하되겠지만, batch size 등을 적당하게 설정해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f52b0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AdamW, get_scheduler, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset\n",
    "\n",
    "# 모델 및 데이터 로드\n",
    "model_name = \"KETI-AIR/ke-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# DataLoader with collator\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=data_collator)\n",
    "val_dataloader = DataLoader(valid_dataset, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "\n",
    "# Optimizer 및 Scheduler 설정\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_training_steps = len(train_dataloader) * 3  # 3 에폭 예시\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "# Accelerator 설정 (GPU 사용)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training Loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        # 배치를 GPU로 이동\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    print(f\"Training loss: {avg_train_loss}\")\n",
    "\n",
    "    # Evaluation Loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_loss += outputs.loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    print(f\"Validation loss: {avg_val_loss}\")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0946eb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "\n",
    "# Load BLEU metric\n",
    "bleu = load(\"bleu\")\n",
    "\n",
    "# Function to calculate BLEU score\n",
    "def compute_bleu_with_evaluate(predictions, references):\n",
    "    \"\"\"\n",
    "    predictions: List of model-generated translations\n",
    "    references: List of reference translations\n",
    "    \"\"\"\n",
    "    results = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "    return results[\"bleu\"]\n",
    "\n",
    "# Evaluation Loop for BLEU Calculation\n",
    "model.eval()\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Generate predictions\n",
    "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=50)\n",
    "        decoded_predictions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        decoded_references = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Collect predictions and references\n",
    "        predictions.extend(decoded_predictions)\n",
    "        references.extend(decoded_references)\n",
    "\n",
    "# Calculate BLEU score\n",
    "bleu_score = compute_bleu_with_evaluate(predictions, references)\n",
    "print(f\"BLEU score: {bleu_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe545112",
   "metadata": {},
   "source": [
    "## 모델 테스트 (Test)\n",
    "- 학습된 모델을 가지고 테스트 데이터로 테스트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92d256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate model on test data\n",
    "def evaluate_model_on_test_data(model, tokenizer, test_dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"]\n",
    "\n",
    "            # Generate predictions\n",
    "            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=50)\n",
    "            decoded_predictions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            decoded_references = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            # Collect predictions and references\n",
    "            predictions.extend(decoded_predictions)\n",
    "            references.extend(decoded_references)\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    bleu_score = compute_bleu_with_evaluate(predictions, references)\n",
    "    return bleu_score, predictions, references\n",
    "\n",
    "\n",
    "# Prepare test DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, collate_fn=data_collator)\n",
    "\n",
    "# Evaluate on test data\n",
    "test_bleu_score, test_predictions, test_references = evaluate_model_on_test_data(model, tokenizer, test_dataloader)\n",
    "\n",
    "# Print the BLEU score\n",
    "print(f\"Test BLEU score: {test_bleu_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280a5dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7c43fc6",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "- Assignment2에 쓰였던 문장들을 이 학습된 모델에서 그 결과를 살펴 보아라\n",
    "\n",
    "- 모든 액체, 젤, 에어로졸 등은 1커트짜리 여닫이 투명봉지 하나에 넣어야 합니다.\n",
    "- 미안하지만, 뒷쪽 아이들의 떠드는 소리가 커서, 광화문으로 가고 싶은데 표를 바꾸어 주시겠어요?\n",
    "- 은행이 너무 멀어서 안되겠네요. 현찰이 필요하면 돈을 훔시세요\n",
    "- 아무래도 분실한 것 같으니 분실 신고서를 작성해야 하겠습니다. 사무실로 같이 가실까요?\n",
    "- 부산에서 코로나 확진자가 급증해서 병상이 부족해지자 확진자 20명을 대구로 이송한다\n",
    "- 변기가 막혔습니다\n",
    "- 그 바지 좀 보여주십시오. 이거 얼마에 살 수 있는 것 입니까?\n",
    "- 비가 와서 백화점으로 가지 말고 두타로 갔으면 좋겠습니다.\n",
    "- 속이 안좋을 때는 죽이나 미음으로 아침을 대신합니다\n",
    "- 문대통령은 집단 이익에서 벗어나라고 말했다\n",
    "- 이것 좀 먹어 볼 몇 일 간의 시간을 주세요\n",
    "- 이 날 개미군단은 외인의 물량을 모두 받아 내었다\n",
    "- 통합 우승의 목표를 달성한 NC 다이노스 나성범이 메이저리그 진출이라는 또 다른 꿈을 향해 나아간다\n",
    "- 이번 구조 조정이 제품을 효과적으로 개발 하고 판매 하기 위한 회사의 능력 강화 조처임을 이해해 주시리라 생각합니다\n",
    "- 요즘 이 프로그램 녹화하며 많은 걸 느낀다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcb47eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentences to translate\n",
    "test_sentences = [\n",
    "    \"모든 액체, 젤, 에어로졸 등은 1커트짜리 여닫이 투명봉지 하나에 넣어야 합니다.\",\n",
    "    \"미안하지만, 뒷쪽 아이들의 떠드는 소리가 커서, 광화문으로 가고 싶은데 표를 바꾸어 주시겠어요?\",\n",
    "    \"은행이 너무 멀어서 안되겠네요. 현찰이 필요하면 돈을 훔치세요.\",\n",
    "    \"아무래도 분실한 것 같으니 분실 신고서를 작성해야 하겠습니다. 사무실로 같이 가실까요?\",\n",
    "    \"부산에서 코로나 확진자가 급증해서 병상이 부족해지자 확진자 20명을 대구로 이송한다.\",\n",
    "    \"변기가 막혔습니다.\",\n",
    "    \"그 바지 좀 보여주십시오. 이거 얼마에 살 수 있는 것 입니까?\",\n",
    "    \"비가 와서 백화점으로 가지 말고 두타로 갔으면 좋겠습니다.\",\n",
    "    \"속이 안좋을 때는 죽이나 미음으로 아침을 대신합니다.\",\n",
    "    \"문대통령은 집단 이익에서 벗어나라고 말했다.\",\n",
    "    \"이것 좀 먹어 볼 몇 일 간의 시간을 주세요.\",\n",
    "    \"이 날 개미군단은 외인의 물량을 모두 받아 내었다.\",\n",
    "    \"통합 우승의 목표를 달성한 NC 다이노스 나성범이 메이저리그 진출이라는 또 다른 꿈을 향해 나아간다.\",\n",
    "    \"이번 구조 조정이 제품을 효과적으로 개발하고 판매하기 위한 회사의 능력 강화 조처임을 이해해 주시리라 생각합니다.\",\n",
    "    \"요즘 이 프로그램 녹화하며 많은 걸 느낀다.\"\n",
    "]\n",
    "\n",
    "# Tokenize test sentences\n",
    "inputs = tokenizer(test_sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Move inputs to the same device as the model\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "# Generate translations\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    translated_ids = model.generate(**inputs, max_length=100, num_beams=4)\n",
    "    translations = tokenizer.batch_decode(translated_ids, skip_special_tokens=True)\n",
    "\n",
    "# Print translations\n",
    "for i, (src, tgt) in enumerate(zip(test_sentences, translations)):\n",
    "    print(f\"Source {i+1}: {src}\")\n",
    "    print(f\"Translation {i+1}: {tgt}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyohyeongjang_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
